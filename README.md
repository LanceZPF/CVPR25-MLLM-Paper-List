# CVPR25-MLLM-Paper-List
CVPR 2025 Multimodal Large Language Models Paper List

## ðŸ“– Table of Contents
- [ðŸ“– Table of Contents](#-table-of-contents)
  - [Image LLMs](#image-llms)
  - [Video LLMs](#video-llms)
  - [Unified LLMs](#unified-llms)
  - [Other Modalities](#other-modalities)
  - [Preferece Optimization](#preference-optimization)
  - [Benchmarks](#benchmarks)
  - [Retrieval](#retrieval)


## Image LLMs
- **Img-Diff**: Contrastive Data Synthesis for Multimodal Large Language Models [Paper](https://arxiv.org/abs/2408.04594) [Code](https://github.com/modelscope/data-juicer/tree/ImgDiff)
- **FlashSloth**: Lightning Multimodal Large Language Models via Embedded Visual Compression [Paper](https://arxiv.org/abs/2412.04317) [Code](https://github.com/codefanw/FlashSloth)
- **BlueLM-V-3B**: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices [Paper](https://arxiv.org/abs/2411.10640v1) [Code]()
- **Insight-V**: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models [Paper](https://arxiv.org/abs/2411.14432) [Code](https://github.com/dongyh20/Insight-V)
- **Mono-InternVL**: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training [Paper](https://arxiv.org/abs/2410.08202) [Code](https://internvl.github.io/blog/2024-10-10-Mono-InternVL/)



## Video LLMs
- **Apollo**: An Exploration of Video Understanding in Large Multi-Modal Models [Paper](https://arxiv.org/abs/2412.10360) [Page](https://apollo-lmms.github.io/)
- **VideoRefer Suite**: Advancing Spatial-Temporal Object Understanding with Video LLM [Paper](https://arxiv.org/abs/2501.00599) {[Code](https://github.com/CircleRadon/VideoRefer-suite)
- **Seq2Time**: Sequential Knowledge Transfer for Video LLM Temporal Grounding [Paper](https://arxiv.org/abs/2411.16932)
- On the Consistency of Video Large Language Models in Temporal Comprehension [Paper](https://arxiv.org/abs/2411.12951) [Code](https://github.com/minjoong507/Consistency-of-Video-LLM)
- **DyCoke**: Dynamic Compression of Tokens for Fast Video Large Language Models [Paper](https://arxiv.org/abs/2411.15024) [Code](https://github.com/KD-TAO/DyCoke)
- **PAVE**: Patching and Adapting Video Large Language Models [Paper](https://drive.google.com/file/d/1whMeSxRh1BiUlunBTz26-7MTjv2K7cRF/view) [Code](https://github.com/dragonlzm/PAVE)
- **DynFocus**: Dynamic Cooperative Network Empowers LLMs with Video Understanding [Paper](https://arxiv.org/abs/2411.12355) [Code](https://github.com/Simon98-AI/DynFocus/tree/main)
- M-LLM Based Video Frame Selection for Efficient Video Understanding [Paper](https://arxiv.org/abs/2502.19680)
- Adaptive Keyframe Sampling for Long Video Understanding [Paper](https://arxiv.org/abs/2502.21271) [Code](https://github.com/ncTimTang/AKS)
- **VISTA**: Enhancing Long-Duration and High-Resolution Video Understanding by Video Spatiotemporal Augmentation [Paper](https://arxiv.org/abs/2412.00927) [Code](https://github.com/TIGER-AI-Lab/VISTA)
- 



## Unified LLMs
- **Janus**: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation [Paper](https://arxiv.org/abs/2410.13848) [Code](https://github.com/deepseek-ai/Janus)

## Other Modalities
- **Thinking in Space**: How Multimodal Large Language Models See, Remember and Recall Spaces [Paper](https://arxiv.org/abs/2412.14171) [Code](https://github.com/vision-x-nyu/thinking-in-space)
- **EventGPT**: Event Stream Understanding with Multimodal Large Language Models [Paper](https://github.com/XduSyL/EventGPT) [Code](https://github.com/XduSyL/EventGPT)

## Preference Optimization
- **Task Preference Optimization**: Improving Multimodal Large Language Models Performance with Vision Task Alignment [Paper](https://github.com/OpenGVLab/TPO) [Code](https://github.com/OpenGVLab/TPO)
- Debiasing Multimodal Large Language Models via Noise-Aware Preference Optimization

## Retrieval
- **GME**: Improving Universal Multimodal Retrieval by Multimodal LLMs [Paper](https://arxiv.org/abs/2412.16855) [Code](https://huggingface.co/Alibaba-NLP/gme-Qwen2-VL-2B-Instruct/blob/main/gme_inference.py)
- Retrieval-Augmented Personalization for Multimodal Large Language Models [Paper](https://arxiv.org/abs/2410.13360) [Code](https://github.com/Hoar012/RAP-MLLM)



## Benchmarks
- **MMVU**: Measuring Expert-Level Multi-Discipline Video Understanding [Paper](https://arxiv.org/abs/2501.12380) [Code](https://github.com/yale-nlp/MMVU)
- **MLVU**: Benchmarking Multi-task Long Video Understanding [Paper](https://arxiv.org/abs/2406.04264) [Code](https://github.com/JUNJIE99/MLVU)
- **OVBench**: How Far is Your Video-LLMs from Real-World Online Video Understanding? [Paper](https://arxiv.org/pdf/2501.05510) [Code](https://github.com/JoeLeelyf/OVO-Bench?tab=readme-ov-file)
- **COUNTS**: Benchmarking Object Detectors and Multimodal Large Language Models under Distribution Shifts
